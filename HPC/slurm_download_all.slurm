#!/bin/bash
#SBATCH --job-name=hf_download
#SBATCH --output=logs/hf_download_%j.out
#SBATCH --error=logs/hf_download_%j.err
#SBATCH --time=10-00:00:00                # Up to 10 days for large datasets
#SBATCH --mem=128G                        # Optimized for  downloader (was 512G)
#SBATCH --cpus-per-task=8                 # Balanced for concurrent downloads
#SBATCH --nice=0
#SBATCH --requeue                         # Allow automatic restart on node failure
#SBATCH --signal=USR1@120                 # Graceful shutdown 2min before kill (was 90s)
#SBATCH --kill-on-invalid-dep=yes
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mail-user=sonnym@hotmail.se

# ğŸ¢ HuggingFace Dataset Downloader - Production Slurm Job


set -euo pipefail
export PYTHONUNBUFFERED=1
function cleanup_and_exit() {
    echo "ğŸ”„ [$(date)] Received shutdown signal - performing graceful cleanup..."
    python -c "import hf_downloader; hf_downloader.cleanup_locks()" 2>/dev/null || true
    echo "âœ… [$(date)] Cleanup completed"
    exit 0
}

function health_check() {
    echo "ğŸ¥ [$(date)] Running  health checks..."
    python -c "
import hf_downloader
import sys

print('ğŸ“‹ System Health Report:')
print(f'   Configuration: {\"âœ…\" if hf_downloader.validate_configuration() else \"âŒ\"}')
print(f'   Network: {\"âœ…\" if hf_downloader.check_network_connectivity() else \"âŒ\"}')
print(f'   Memory: {hf_downloader.check_memory_usage():.1f}%')
print(f'   Cache: {\"âœ…\" if hf_downloader.check_cache_permissions() else \"âŒ\"}')
print(f'   Disk Space: {\"âœ…\" if hf_downloader.check_disk_space(100.0) else \"âŒ\"}')

# Exit with error if any critical checks fail
if not all([
    hf_downloader.validate_configuration(),
    hf_downloader.check_network_connectivity(),
    hf_downloader.check_cache_permissions(),
    hf_downloader.check_disk_space(100.0)
]):
    print('âŒ Critical health checks failed')
    sys.exit(1)
else:
    print('âœ… All health checks passed - ready for production')
"
}


trap cleanup_and_exit USR1 TERM INT

echo "ğŸ¢ =====  HF DATASET DOWNLOADER ====="
echo "ğŸš€ Starting production dataset download pipeline"
echo "ğŸ“… Job started: $(date)"
echo "ğŸ§  Compute node: $(hostname)"
echo "ğŸ”’ SLURM_JOB_ID: ${SLURM_JOB_ID}"
echo "ğŸ“Š Resources: ${SLURM_MEM_PER_NODE}MB RAM, ${SLURM_CPUS_PER_TASK} CPUs"
echo "â° Time limit: ${SLURM_TIME_LIMIT}"
echo "ğŸ“§ Notifications: ${SLURM_MAIL_USER}"
echo ""
echo "ğŸ”§ Setting up  environment..."


if command -v module &> /dev/null; then
    module load anaconda 2>/dev/null || echo "âš ï¸  Warning: anaconda module not found"
fi


# ğŸ”‘ HuggingFace Token Configuration
# Option 1: Use token from environment (recommended)
if [[ -z "${HF_TOKEN:-}" ]]; then
    echo "âš ï¸  Warning: HF_TOKEN not set - some datasets may require authentication"
    echo "ğŸ’¡ Set HF_TOKEN environment variable or run 'huggingface-cli login'"
else
    echo "âœ… HF_TOKEN configured"
    export HF_TOKEN="$HF_TOKEN"
fi

# Option 2: Set token directly in script
export HF_TOKEN="hf_your_token_here"  # ğŸ”„ REPLACE WITH YOUR ACTUAL TOKEN

# Optional: Set custom cache directory
# export HF_HOME="/path/to/your/cache"

# Activate environment (update this to your actual environment name)
ENV_NAME="tokenizer"  # ğŸ”„ UPDATE THIS TO YOUR ACTUAL ENVIRONMENT NAME
if [[ -n "${CONDA_DEFAULT_ENV:-}" ]]; then
    echo "ğŸ“¦ Current conda environment: $CONDA_DEFAULT_ENV"
else
    echo "ğŸ“¦ Activating conda environment: $ENV_NAME"
    source activate "$ENV_NAME" || {
        echo "âŒ Failed to activate environment '$ENV_NAME'"
        echo "ğŸ’¡ Please update ENV_NAME in this script to your actual environment"
        exit 1
    }
fi


echo "ğŸ Python version: $(python --version)"
echo "ğŸ“ Python location: $(which python)"


echo ""
echo "ğŸ“ Setting up  directory structure..."
mkdir -p logs datasets datasets/_locks datasets/_status


chmod 755 logs datasets datasets/_locks datasets/_status 2>/dev/null || true


echo ""
health_check


echo ""
echo "â¬‡ï¸  Starting  download pipeline..."
echo "ğŸ”’ Security: Whitelist validation enabled"
echo "ğŸ›¡ï¸  Reliability: Exponential backoff with jitter"
echo "ğŸ“Š Monitoring: Real-time resource tracking"
echo "ğŸ”„ Recovery: Automatic retry with lock management"


echo ""
echo "ğŸ“¥ Phase 1:  Dataset Download"
python hf_downloader.py --download-only --force

DOWNLOAD_EXIT_CODE=$?
echo "ğŸ“Š Download phase exit code: $DOWNLOAD_EXIT_CODE"


if [ $DOWNLOAD_EXIT_CODE -eq 0 ]; then
    echo ""
    echo "ğŸ” Phase 2:  Dataset Verification"
    python hf_downloader.py --verify
    VERIFY_EXIT_CODE=$?
    echo "ğŸ“Š Verification phase exit code: $VERIFY_EXIT_CODE"
else
    echo "âš ï¸  Skipping verification due to download errors"
    VERIFY_EXIT_CODE=1
fi


echo ""
echo "ğŸ§¹ Phase 3:  Cleanup"
python hf_downloader.py --cleanup


echo ""
echo "ğŸ“ˆ ===== COMPLETION REPORT ====="
echo "ğŸ“… Job completed: $(date)"
echo "â±ï¸  Total runtime: $SECONDS seconds"


python -c "
import hf_downloader
import json

try:
    status = hf_downloader.load_status()
    total = len(status)
    done = sum(1 for v in status.values() if v == 'done')
    failed = sum(1 for v in status.values() if v == 'failed')
    corrupt = sum(1 for v in status.values() if v == 'corrupt')
    
    print(f'ğŸ“Š Dataset Status Summary:')
    print(f'   Total datasets: {total}')
    print(f'   âœ… Successfully downloaded: {done}')
    print(f'   âŒ Failed downloads: {failed}')
    print(f'   ğŸ”§ Corrupt/needs repair: {corrupt}')
    print(f'   ğŸ“ˆ Success rate: {(done/total*100):.1f}%' if total > 0 else '   ğŸ“ˆ Success rate: N/A')
    
    if failed > 0 or corrupt > 0:
        print(f'âš ï¸  Issues detected - review logs for details')
        
except Exception as e:
    print(f'âŒ Error generating status report: {e}')
"


if [ $DOWNLOAD_EXIT_CODE -eq 0 ]; then
    echo "ğŸ‰  DOWNLOAD PIPELINE COMPLETED SUCCESSFULLY"
    echo "âœ… All datasets processed with -grade reliability"
    exit 0
else
    echo "âš ï¸   DOWNLOAD PIPELINE COMPLETED WITH ISSUES"
    echo "ğŸ“‹ Check logs for detailed error analysis"
    echo "ğŸ”„ Job may be automatically requeued by Slurm"
    exit $DOWNLOAD_EXIT_CODE
fi
